# -*- coding: utf-8 -*-
"""odinmlp1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15mUP8O1qCL2k1eseDmvB2SuAnWQDzzZZ

# Introduction

A bank's credit card department is one of the top adopters of data science. A top focus for the bank has always been acquiring new credit card customers. Giving out credit cards without doing proper research or evaluating applicants' creditworthiness is quite risky. The credit card department has been using a data-driven system for credit assessment called Credit Scoring for many years, and the model is known as an application scorecard. A credit card application's cutoff value is determined using the application scorecard, which also aids in estimating the applicant's level of risk. This decision is made based on strategic priority at a given time.


Customers must fill out a form, either physically or online, to apply for a credit card. The application data is used to evaluate the applicant's creditworthiness. The decision is made using the application data in addition to the Credit Bureau Score, such as the FICO Score in the US or the CIBIL Score in India, and other internal information on the applicants. Additionally, the banks are rapidly taking a lot of outside data into account to enhance the caliber of credit judgements.

Features name: (Credit_Card.csv)

Ind_ID: Client ID

Gender: Gender information

Car_owner: Having car or not

Propert_owner: Having property or not

Children: Count of children

Annual_income: Annual income

Type_Income: Income type

Education: Education level

Marital_status: Marital_status

Housing_type: Living style

Birthday_count: Use backward count from current day (0), -1 means yesterday.

Employed_days: Start date of employment. Use backward count from current day (0). Positive value means, individual is currently unemployed.

Mobile_phone: Any mobile phone

Work_phone: Any work phone

Phone: Any phone number

EMAIL_ID: Any email ID

Type_Occupation: Occupation

Family_Members: Family size

# Questions to Answer

* Using credit card approval to predict potentially risky customers helps minimize an organization's risk. The better their prediction, the less loss they will suffer. However, instead of solely focusing on excluding high-risk customers, organizations can also strategize on attracting and retaining good customers to further contribute to profitability.

* Customers can be further grouped based on their background, history, economic health, and other factors, reducing risk for the organization. Good customers, meanwhile, can be given preferential treatment, which can encourage good retention rates and build loyalty.

* While credit scores provide a valuable benchmark, a more detailed analysis of individual circumstances can offer nuanced insights. Additionally, the dynamic nature of individual financial situations allows for updating risk assessments over time, providing a more accurate understanding of each customer's potential.

* Overall, while credit score and previous records are necessary for initial risk assessment, a comprehensive approach that considers both high-risk and low-risk customer segments, along with ongoing individual analysis, can lead to optimal credit card portfolio management and increased profitability.

* I would also like to rephrase the question about the type of work people do, as many of the values in that field are missing. This is a major issue and needs to be addressed.

#  Initial Hypothesis

* The logistic algorithm is going to give better results than the other ML models for this test.

* There may be a positive association between family members and the number of children
"""

correlation = df['CHILDREN'].corr(df['Family_Members'])
correlation

"""These two columns are very highly co related .One should be removed

# Data importation

## importing necessary libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')

label=pd.read_csv('/content/Credit_card_label.csv')

normal_data=pd.read_csv('/content/Credit_card.csv')

"""# EDA"""

label.head()

"""Label:

- 0 is application approved
- 1 is application rejected
"""

normal_data.head()

"""We have two dataset with us

1) with Dependent Variables.

2) with Independent Variables

joining these two to get an overall view.Both have Ind_ID common in them
"""

merged_data = pd.merge(normal_data, label, on='Ind_ID', how='left')

merged_data.head()

normal_data.shape

merged_data.drop('Ind_ID',axis=1,inplace=True)

normal_data.shape

merged_data.info()

merged_data.describe(include='all')

merged_data.drop_duplicates(inplace=True)

merged_data.shape

"""## Data Preprocessing"""

merged_data.isnull().sum()

"""Checking the null values"""

merged_data.isnull().mean()*100

"""Missing values in the below features:

*  Gender
* Annual_income
* Birthday_count
* Type_occupation
"""

!pip install missingno
import missingno as msno

msno.bar(merged_data)

msno.matrix(merged_data)

"""## Handeling the missing value

Data is missing at random and except Type_Occupation all are very less so can completely dropped them
"""

merged_data.dropna(axis=0,subset=['GENDER','Annual_income','Birthday_count'],how='any',inplace=True)

merged_data.shape

"""I started with 1548 columns.After dropping duplicates 1386 columns remained .finally after dropping null i got 1334"""

merged_data['Type_Occupation'].value_counts()

df=merged_data

df.info()

df.head()

"""Majority of features are catagorical so reviewing them in bar chart"""

toview = ['GENDER', 'Car_Owner','CHILDREN','Propert_Owner','Type_Income','EDUCATION','Marital_status','Housing_type','Type_Occupation']
for col in toview:
  plt.figure(figsize=(15,7))
  sorted_counts=df[col].value_counts().sort_values(ascending=False)
  sns.countplot(x=df[col],order=sorted_counts.index,palette='plasma')

"""* Majority of the data is taken from female
* Most of the people who were applying do not own a car
* Most people don't have any children ,for one person is showing 14 children ,clearly a outlier
* Most individuals in the dataset are property owners, it might be play a critical role in credit card approval system .
* In Income type most of the people are of working catogery type follwed by commercial associate ,pensioner and state servent
* Most of the people have done secondary special or higher very few are below that or have a higher education
* Majority of indivisual are married wile rest are single,divorced,seperated .For better processing i have divided these into two catagories either married or not married .later can be converted to onehot encoding
* Same case here.More than 95% people are living in House/Apartment so it would not be decinding factor
* I am planning to drop this column as there are too many missing value and there a varity of catagories are there
"""

df.head()

"""Birthday_count is in negative number and is represented in days so converting it to yearwise"""

df['Birthday_count']=round(df['Birthday_count']/-365)

import copy
sql=copy.deepcopy(df)

"""Going to change the data for ml project so need a copy in case want to revert it and later for sql part"""

df['Employed_days']=np.where(df['Employed_days']<0,1,0)

df['GENDER']=np.where(df['GENDER']=='M',1,0)

df['Car_Owner']=np.where(df['Car_Owner']=='Y',1,0)

df['Propert_Owner']=np.where(df['Propert_Owner']=='Y',1,0)

df['Marital_status'].value_counts()

df['EDUCATION'].value_counts()

"""person with a family have a high chance of getting credit card approved .Although onehot encoding can also be applicable here I will try if i am not getting good result."""

df['Marital_status']=np.where(df['Marital_status']=='Married',1,0)

plt.subplots(figsize=(8,8))
sns.heatmap(df.corr(),annot=True)
plt.show()

"""## Dropping some columns"""

df['Mobile_phone'].value_counts()

"""dropping the data

---



*   everyone has mobile phone so no point keeping it
*   children is highly co related to family member so dropping the children column
*   type occupation has too many missing value so dropping it




   


"""

df.drop(columns=['Mobile_phone','CHILDREN','Type_Occupation'],inplace=True)

"""as children is highly co related to family member"""

df.head()

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot the histogram on the first subplot.
sns.histplot(df['Annual_income'], kde='True', ax=axes[0])
axes[0].set_title('Histogram of Annual Income')

# Plot the boxplot on the second subplot.
sns.boxplot(df['Annual_income'], ax=axes[1])
axes[1].set_title('Boxplot of Annual Income')

# Show the plot.
plt.show()

"""## Removing outlier

its a right skewed data suggesting that majority of the people are in low income catagory .Log trasformation can be done in it to make it normally distributed data
"""

# Finding the IQR
percentile25 = df['Annual_income'].quantile(0.25)
percentile75 = df['Annual_income'].quantile(0.75)
iqr = percentile75 - percentile25

upper_limit = percentile75 + 1.5 * iqr
lower_limit = percentile25 - 1.5 * iqr

print("Upper limit",upper_limit)
print("Lower limit",lower_limit)

df[df['Annual_income'] > upper_limit].shape

df['Annual_income']=np.where(df['Annual_income']>upper_limit,upper_limit,df['Annual_income'])

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot the histogram on the first subplot.
sns.histplot(df['Annual_income'], kde='True', ax=axes[0])
axes[0].set_title('Histogram of Annual Income')

# Plot the boxplot on the second subplot.
sns.boxplot(df['Annual_income'], ax=axes[1])
axes[1].set_title('Boxplot of Annual Income')

# Show the plot.
plt.show()

"""It is not exactly a normal distribution but close to it"""

# from sklearn import preprocessing
# minmax = preprocessing.MinMaxScaler()
# minmax.fit_transform(df[['Annual_income']])

df.head()

plt.subplots(figsize=(8,8))
sns.heatmap(df.corr(),annot=True)
plt.show()

from sklearn.preprocessing import OrdinalEncoder

education_encoder = OrdinalEncoder(categories=[['Lower secondary','Secondary / secondary special','Incomplete higher','Higher education','Academic degree']])
df['EDUCATION'] = education_encoder.fit_transform(df[['EDUCATION']])

# prompt: sns.histplot(df['Annual_income'],kde='True') sns.boxplot(df['Annual_income'])  plot these two on same plot

import matplotlib.pyplot as plt
import seaborn as sns

# Create a figure with two subplots.
fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Plot the histogram on the first subplot.
sns.histplot(df['Annual_income'], kde='True', ax=axes[0])
axes[0].set_title('Histogram of Annual Income')

# Plot the boxplot on the second subplot.
sns.boxplot(df['Annual_income'], ax=axes[1])
axes[1].set_title('Boxplot of Annual Income')

# Show the plot.
plt.show()

df.Type_Income.value_counts()  # needs one hot encoding

df.Housing_type.value_counts()  # needs one hot encoding

sns.histplot(df.Birthday_count,kde=True) # looks ok

sns.boxplot(df.Birthday_count)

plt.figure(figsize =(5,4))

sns.boxplot(data=df , x='Family_Members')

plt.grid()
plt.show()

# removing the exterme value

df = df[df['Family_Members'] < 14]

df.head()

"""## Feature Transformation"""

# columns to onehot encoding
one_cols = df[['Type_Income','Housing_type']]

df = pd.get_dummies(df, columns=['Type_Income','Housing_type'],drop_first=True)  # drop_first to avoid multicolinearity

df.head()

"""## Normalization"""

from sklearn import preprocessing
minmax = preprocessing.MinMaxScaler()
minmax.fit_transform(df[['Annual_income','Birthday_count']])

onehot=pd.DataFrame(minmax.fit_transform(df[['Annual_income','Birthday_count']]),columns=['Annual_income','Birthday_count'])

onehot

df=df.drop(columns=['Annual_income','Birthday_count'])

df.isnull().sum()

df.reset_index()

final_df=pd.concat([df.reset_index(),onehot],axis=1)

final_df.sample(10)

df=final_df.drop(columns=['index'])

df

"""## Dealing with Imabalance in Dataset"""

df.label.value_counts()

X = df.drop(columns=['label'])
y = df['label']

# importing SMOTE
from imblearn.over_sampling import SMOTE

oversample = SMOTE()

X, y = oversample.fit_resample(X, y)

y.value_counts()

"""# Model implimanting

It is a classification problem 1 representing approved and 0 representing rejected
"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.20, random_state=1)

from sklearn.model_selection import GridSearchCV, StratifiedKFold

"""## svm"""

from sklearn import svm
model = svm.SVC()

parameters_grid = {'C': [0.1, 1, 10, 100], # Large C -> small margin less mis classified; Small C - large margin more mis classifications
                   'gamma': [1, 0.1, 0.01], # Bigger Gamma more linear boundary; smaller gamma more complex model
                   'kernel': ['rbf']}

skf = StratifiedKFold(n_splits=4)
grid = GridSearchCV(estimator = model,
                    param_grid = parameters_grid,
                    scoring = 'precision', # = (TPR + TFR)/2.0
                    cv = skf,
                    verbose = 1) #use 3

gridResult = grid.fit(X_train, y_train)
print(f"Best result = {gridResult.best_score_:1.2f} and parameters = {gridResult.best_params_}")

finalModel = gridResult.best_estimator_
finalModel.fit(X_train, y_train)
y_pred= finalModel.predict(X_test)

# y_pred

y_pred_train = finalModel.predict(X_train)
y_pred_test  = finalModel.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_test))
print(classification_report(y_train, y_pred_train))

data = [["svm", 0.87]]
result = pd.DataFrame(data, columns=["Model", "Test Accuracy"])

"""## Random forest"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()

# defining parameter range
parameters_grid = {'criterion': ['entropy', 'gini', 'log_loss'],
                   'max_depth': [3,5,6], # Max : sqrt(Number of Features) : applies for datases with more columns
                   'n_estimators': [100, 150],
                   'min_samples_leaf' : [10,20]
                  }

skf = StratifiedKFold(n_splits=4)
grid = GridSearchCV(estimator = model,
                    param_grid = parameters_grid,
                    scoring = 'balanced_accuracy', # = (TPR + TFR)/2.0
                    cv = skf,
                    verbose = 1)

gridResult = grid.fit(X_train, y_train)
print(f"Best result = {gridResult.best_score_:1.2f} and parameters = {gridResult.best_params_}")

# finalModel = RandomForestClassifier(n_estimators= 100, criterion="log_loss", max_depth = 6)
finalModel = gridResult.best_estimator_
finalModel.fit(X_train, y_train)
y_pred= finalModel.predict(X_test)

# y_pred

y_pred_train = finalModel.predict(X_train)
y_pred_test  = finalModel.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_test))
print(classification_report(y_train, y_pred_train))

result.loc[1] = ["Random Forest", 0.75]

"""## knnclassifier"""

from sklearn.neighbors import KNeighborsClassifier
model= KNeighborsClassifier()

# defining parameter range
parameters_grid = {'n_neighbors': [1,3],
                   'metric':['minkowski'],
                  'p': [1,2,3],
                  'weights':['uniform', 'distance']
                  }

skf = StratifiedKFold(n_splits=4)
grid = GridSearchCV(estimator = model,
                    param_grid = parameters_grid,
                    scoring = 'balanced_accuracy', # = (TPR + TFR)/2.0
                    cv = skf,
                    verbose = 1) #use 3

# fitting the model for grid search

gridResult = grid.fit(X_train, y_train)
print(f"Best result = {gridResult.best_score_:1.2f} and parameters = {gridResult.best_params_}")

finalModel = gridResult.best_estimator_
finalModel.fit(X_train, y_train)
y_pred= finalModel.predict(X_test)

# y_pred

y_pred_train = finalModel.predict(X_train)
y_pred_test  = finalModel.predict(X_test)

print(classification_report(y_test, y_pred_test))
print(classification_report(y_train, y_pred_train))

result.loc[2] = ["knn", 0.85]

"""In knn classifier model perform well on train data .As these are the data where knn get trained

## xgboost
"""

import xgboost as xgb

#For classification
model = xgb.XGBClassifier(objective ='binary:logistic')



parameters_grid = {'criterion': ['entropy', 'gini', 'log_loss'],
                   'max_depth': [3,5,6],
                   'n_estimators': [50,60],
                   'learning_rate' : [0.05, 0.1, 0.2],
                   'colsample_bytree' : [0.3, 0.5],
                   'alpha' : [1,2]
                  }

skf = StratifiedKFold(n_splits=4)
grid = GridSearchCV(estimator = model,
                    param_grid = parameters_grid,
                    scoring = 'balanced_accuracy', # = (TPR + TFR)/2.0
                    cv = skf,
                    verbose = 1) #use 3

# fitting the model for grid search

gridResult = grid.fit(X_train, y_train)
print(f"Best result = {gridResult.best_score_:1.2f} and parameters = {gridResult.best_params_}")

finalModel = gridResult.best_estimator_
finalModel.fit(X_train, y_train)
y_pred= finalModel.predict(X_test)

# y_pred
print(classification_report(y_test, y_pred_test))
print(classification_report(y_train, y_pred_train))

result.loc[3] = ["xgboost", 0.85]

"""## logistic regrssion"""

from sklearn.linear_model import LogisticRegression

logistic_reg = LogisticRegression(random_state = 0)

logistic_reg.fit(X_train,y_train)

logistic_reg_pred=logistic_reg.predict(X_test)

print(classification_report(y_test, logistic_reg_pred))

result.loc[4] = ["Logistic regression", 0.70]

result

sns.barplot(data=result,x='Model',y='Test Accuracy',palette="Set1")
plt.show()

"""This project focuses on a credit card approval system that evaluates applicants based on various features. During development, we encountered several data pre-processing steps, including feature scaling, label encoding, missing value handling, and feature transformation. We then implemented and optimized five different machine learning models with hyperparameter tuning. Finally, we assessed their performance using the accuracy score.

For this credit card approval classification task , SVM achieved the best accuracy, followed by KNN and XGBoost, contrary to my expectation that logistic regression would excel

# SQL

Use MySQL or PyMySQL to perform the below queries.

Note: Use only the cleaned data for SQL part of the project


Group the customers based on their income type and find the average of their annual income.

Find the female owners of cars and property.

Find the male customers who are staying with their families.

Please list the top five people having the highest income.

How many married people are having bad credit?

What is the highest education level and what is the total count?

Between married males and females, who is having more bad credit?
"""

import duckdb
df = pd.DataFrame(sql)
duckdb.sql("SELECT * FROM df")

"""1) Group the customers based on their income type and find the average of their annual income ?"""

duckdb.sql("SELECT avg(Annual_income) average_income,Type_Income class_type FROM df group by Type_Income")

"""2) Find the female owners of cars and property ?


"""

duckdb.sql("SELECT count(*)  female_owners_of_cars_and_property FROM df where GENDER='F' and Car_Owner='Y' ")

"""3) Find the male customers who are staying with their families ?"""

duckdb.sql("SELECT count(*)  male_who_are_staying_with_families FROM df where GENDER='M' and Family_Members !=0 ")

"""4) Please list the top five people having the highest income.


"""

duckdb.sql("SELECT Annual_income top_five_highest_income FROM df order by Annual_income desc limit 5 ")

"""5) How many married people are having bad credit?"""

duckdb.sql("SELECT count(*) married_people_having_bad_credit FROM df where label=0 and GENDER='M' ")

"""6) What is the highest education level and what is the total count?"""

duckdb.sql("SELECT EDUCATION, count(*)  total_no  FROM df group by EDUCATION")

"""There are 2 people with highest education (Academic degree)

7) Between married males and females, who is having more bad credit?
"""

duckdb.sql("SELECT GENDER, count(*)  total_no  FROM df where Marital_status='Married'and label=0 group by GENDER")

"""females are having bad credit"""